### 英语学习

#### 3. The Programming Interface
```
What functions do we need an operating system to provide applications?
1. Process management. Can a program create an instance of another program? Wait
for it to complete? Stop or resume another running program? Send it an asynchronous
event?
2. Input/output. How do processes communicate with devices attached to the computer
and through them to the physical world? Can processes communicate with each
other?
3. Thread management. Can we create multiple activities or threads that share memory
or other resources within a process? Can we stop and start threads? How do we
synchronize their use of shared data structures?
4. Memory management. Can a process ask for more (or less) memory space? Can it
share the same physical memory region with other processes?
5. File systems and storage. How does a process store the user’s data persistently so
that it can survive machine crashes and disk failures? How does the user name and
organize their data?
6. Networking and distributed systems. How do processes communicate with
processes on other computers? How do processes on different computers coordinate
their actions despite machine crashes and network problems?
7. Graphics and window management. How does a process control pixels on its
portion of the screen? How does a process make use of graphics accelerators?
8. Authentication and security. What permissions does a user or a program have, and
how are these permissions kept up to date? On what basis do we know the user (or
program) is who they say they are?

persistently
美[pɚˈsɪstəntlɪ]
adv. 坚持地；固执地

coordinate
美[ koʊˈɔːrdɪneɪt]
v. (使)协调；搭配

despite
美[dɪˈspaɪt]
prep. 尽管

portion
美[ˈpɔːrʃn]
n. 一部分
```

```
3.1 Process Management

A different approach is to allow user programs to create and manage their own processes.
This has fostered a blizzard of innovation. Today, programs that create and manage
processes include window managers, web servers, web browsers, shell command line
interpreters, source code control systems, databases, compilers, and document
preparation systems. We could go on, but you get the idea. If creating a process is
something a process can do, then anyone can build a new version of any of these
applications, without recompiling the kernel or forcing anyone else to use it.

approach
美[əˈproʊtʃ]
v. 对付，处理

foster
美[ˈfɑːstər]
v. 促进，鼓励

innovation
美[ˌɪnəˈveɪʃn]
n. 改革，创新

interpreter
美[ɪnˈtɜːrprətər]
n. 解释者
```

```
3.1.1 Windows Process Management

Unfortunately, there are quite a few aspects of the process that the parent might like to
control, such as: its privileges, where it sends its input and output, what it should store its
files, what to use as a scheduling priority, and so forth. We cannot trust the child process
itself to set its own privileges and priority, and it would be inconvenient to expect every
application to include code for figuring out its context. So the real interface to
CreateProcess is quite a bit more complicated in practice.

privilege
美[ˈprɪvəlɪdʒ]
n. 特权

inconvenient
美[ˌɪnkənˈvinjənt]
adj. 不方便的；打扰人的

complicated
美[ˈkɑːmplɪkeɪtɪd]
adj. 复杂的
```

```
3.1.2 UNIX Process Management

Further, in both Windows and UNIX, handles are reference counted. Whenever the kernel returns a handle, it bumps a reference counter, and whenever the process releases a handle (or exits), the reference counter is decremented. UNIX fork sets the process ID reference count to two, one for the parent and one for the child. The underlying data structure, the PCB, is reclaimed only when the reference count goes to zero, that is, when both the parent and child terminate.

bump
美[bʌmp]
v. 碰撞；引起

reclaim
美[rɪˈkleɪm]
vt. 回收再利用
```

```
3.2 Input/Output

The Internet has a similar facility to UNIX pipes called TCP (Transmission Control
Protocol). Where UNIX pipes connect processes on the same machine, TCP provides
a bi-directional pipe between two processes running on different machines. In TCP,
data is written as a sequence of bytes on one machine and read out as the same
sequence on the other machine.

facility
美[fəˈsɪləti]
n. 设施

bi-directional
美['bɪdɪr'ekʃənl]
adj. 双流向的

sequence
美[ˈsiːkwəns]
n. [数]数列，序列
```

```
3.3 Case Study: Implementing a Shell

use a pipe to connect two programs together, so that the output of one is the input of
another. This is called a producer-consumer relationship. For example, in the C-compiler, 
the output of the preprocessor is sent to the parser, and the output of the
parser is sent to the code-generator and then to the assembler. In the standard UNIX
shell, a pipe connecting two programs is signified by a “|” symbol, as in: “cpp file.c |
cparse | cgen | as > file.o”. In this case the shell creates four separate child processes,
each connected by pipes to its predecessor and successor. Each of the phases can
run in parallel, with the parent waiting for all of them to finish.

signified
美[ˈsɪɡnəˌfaɪd]
v. 表示…的意思

phase
美[feɪz]
n. 阶段

parallel
美[ˈpærəlel]
adj. 并行的
```

```
3.4 Case Study: Interprocess Communication

Three widely used forms of interprocess communication are:
1. Producer-consumer.
2. Client-server.
3. File system.
```

```
3.4.1 Producer-Consumer Communication

In UNIX, when the producer finishes, it closes its side of the pipe, but there may still be
data queued in the kernel for the consumer. Eventually, the consumer reads the last of the
data, and the read system call will return an “end of file” marker. Thus, to the consumer,
there is no difference between reading from a pipe and reading from a file.

eventually
美[ɪˈventʃuəli]
adv. 最后
```

```
3.4.2 Client-Server Communication

We can take this streamlining even further. On a multicore system, it is possible or even likely that both the client and server each have their own processor. If the kernel sets up a shared memory region accessible to both the client and the server and no other processes, then the client and server can (safely) pass requests and replies back and forth, as fast as the memory system will allow, without ever traversing into the kernel or relinquishing their processors.

streamlining
美['strimlaɪnɪŋ]
n. 流程

traverse
美[ˈtrævɜːrs]
n. 穿越

relinquish
美[rɪˈlɪŋkwɪʃ]
vt. 放弃；让出
```

```
3.5 Operating System Structure

Inside the operating system, there is often quite frequent interaction between these modules:
1. Many parts of the operating system depend on synchronization primitives for
coordinating access to shared data structures with the kernel.
2. The virtual memory system depends on low-level hardware support for address
translation, support that is specific to a particular processor architecture.
3. Both the file system and the virtual memory system share a common pool of blocks of
physical memory. They also both depend on the disk device driver.
4. The file system can depend on the network protocol stack if the disk is physically
located on a different machine.

synchronization
美[ˌsɪŋkrənaɪ'zeɪʃn]
n. 同一时刻

primitive
美[ˈprɪmətɪv]
n. 原语

coordinating
美[ koˈɔrdɪnetɪŋ]
v. 使协调

specific
美[spəˈsɪfɪk]
adj. 明确的；独特的

particular
美[pərˈtɪkjələr]
adj. 特定的；特殊的
```

```
3.5.1 Monolithic Kernels

Internal to a monolithic kernel, the operating system designer is free to develop whatever
interfaces between modules that make sense, and so there is quite a bit of variation from
operating system to operating system in those internal structures. However, two common
themes emerge across systems: to improve portability, almost all modern operating
systems have both a hardware abstraction layer and dynamically loaded device drivers.

internal
美[ɪnˈtɜːrnl]
adj. 内部的

monolithic
美[ˌmɑnəˈlɪθɪk]
adj. 整体的；庞大的

variation
美[ˌveriˈeɪʃn]
n. 变化

emerge
美[iˈmɜːrdʒ]
v. 出现，兴起
```

```
3.5.2 Microkernel

A microkernel design offers considerable benefit to the operating system developer, as it
easier to modularize and debug user-level services than kernel code. Aside from a
potential reliability improvement, however, microkernels offer little in the way of visible
benefit to end users and can slow down overall performance by inserting extra steps
between the application and the services it needs. Thus, in practice, most systems adopt a
hybrid model where some operating system services are run at user-level and some are in
the kernel, depending on the specific tradeoff between code complexity and performance.

considerable
美[kənˈsɪdərəbl]
adj. 相当多的

modularize
美['mɒdʒələˌraɪz]
v. 模块化

potential
美[pəˈtenʃl]
adj. 潜在的

adopt
美[ əˈdɑːpt]
v. 采用

complexity
美[kəmˈpleksəti]
n. 复杂性
```

```
3.6 Summary and Future Directions

Of course, the reality is often quite different. An alternate model is for operating systems to
divide resources among applications and then allow each application to decide for itself
how best to use those resources. One can think of this as a type of federalism. If both the
operating system and applications are governments doing their own resource allocation,
they are likely to get in each other’s way if they are not careful. As a simple example,
consider how a garbage collector works; it assumes it has a fixed amount of memory to
manage. However, as other applications start or stop, it can gain or lose memory, and if the
operating system does this reallocation transparently, the garbage collector has no hope of
adapting. We will see examples of this same design pattern in many different areas of
operating system design.

reality
美[riˈæləti]
n. 现实

divide
美[dɪˈvaɪd]
v. 分开；分配

federalism
英['fedərəlɪzəm]
n. 联邦政治
```

```
8. Address Translation

Chapter roadmap:
1. Address Translation Concept. We start by providing a conceptual framework for
understanding both hardware and software address translation. (Section 8.1)
2. Flexible Address Translation. We focus first on hardware address translation; we
ask how can we design the hardware to provide maximum flexibility to the operating
system kernel? (Section 8.2)
3. Efficient Address Translation. The solutions we present will seem flexible but terribly
slow. We next discuss mechanisms that make address translation much more efficient,
without sacrificing flexibility. (Section 8.3)
4. Software Protection. Increasingly, software compilers and runtime interpreters are
using address translation techniques to implement operating system functionality. 
What changes when the translation is in software rather than in hardware?

flexible
美[ˈfleksəbl]
adj. 灵活的

present
美[ˈpreznt]
v. 展示，呈现

mechanism
美[ˈmekənɪzəm]
n. 机制

sacrifice
美[ˈsækrɪfaɪs]
v. 牺牲
```

```
8.1 Address Translation Concept

Today, we still have something similar. Complex programs often have multiple files, each of which can be compiled independently and then linked together to form the executable image. When the compiler generates the machine instructions for a single file, it cannot know where in the executable this particular file will go. Instead, the compiler generates a symbol table at the beginning of each compiled file, indicating which values will need to be modified when the individual files are assembled together.

assemble
美[əˈsembl]
v. 聚集
```

```
8.2 Towards Flexible Address Translation

Base and bounds translation is both simple and fast, but it lacks many of the features
needed to support modern programs. Base and bounds translation supports only coarse-grained 
protection at the level of the entire process; it is not possible to prevent a program
from overwriting its own code, for example. It is also difficult to share regions of memory
between two processes. Since the memory for a process needs to be contiguous,
supporting dynamic memory regions, such as for heaps, thread stacks, or memory mapped
files, becomes difficult to impossible.

bound
美[baʊnd]
n. 界限，限制

coarse
美[kɔːrs]
adj. 粗糙的
```

```
8.2.1 Segmented Memory

However we choose to place new segments, as more memory becomes allocated, the
operating system may reach a point where there is enough free space for a new segment,
but the free space is not contiguous. This is called external fragmentation. The operating
system is free to compact memory to make room without affecting applications, because
virtual addresses are unchanged when we relocate a segment in physical memory. Even
so, compaction can be costly in terms of processor overhead: a typical server configuration
would take roughly a second to compact its memory.

contiguous
美[kənˈtɪɡjuəs]
adj. 邻近的

external
美[ɪkˈstɜːrnl]
n. 外部

fragmentation
英[ˌfræɡmen'teɪʃn]
n. 碎片

compact
美[ˈkɑːmpækt]
v. 把…压实
```

```
8.2.2 Paged Memory

A downside of paging is that while the management of physical memory becomes simpler,
the management of the virtual address space becomes more challenging. Compilers
typically expect the execution stack to be contiguous (in virtual addresses) and of arbitrary
size; each new procedure call assumes the memory for the stack is available. Likewise, the
runtime library for dynamic memory allocation typically expects a contiguous heap. In a
single-threaded process, we can place the stack and heap at opposite ends of the virtual
address space, and have them grow towards each other, as shown in Figure 8.5. However,
with multiple threads per process, we need multiple thread stacks, each with room to grow.

downside
美[ˈdaʊnsaɪd]
n. 负面

arbitrary
美[ˈɑːrbɪtreri]
adj. 任意的
```












